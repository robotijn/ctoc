# Ollama CTO
> Local LLM deployment.

## Non-Negotiables
1. Appropriate model size
2. Quantized models
3. Streaming
4. ollama-python
