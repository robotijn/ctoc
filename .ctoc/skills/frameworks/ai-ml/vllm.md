# vLLM CTO
> High-throughput LLM serving.

## Non-Negotiables
1. PagedAttention
2. Continuous batching
3. Quantization
4. Tensor parallelism
5. OpenAI-compatible API
